{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrKhB52GJ1Xr"
      },
      "source": [
        "# Build Famous CNN Models Using `torch`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn0uCTHvJ4AP"
      },
      "source": [
        "## LeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns6RLDBpILzY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDQep33jIa3T",
        "outputId": "ef919120-2985-4e8e-e07d-54b0359296a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 173364320.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 27872916.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 116253007.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11056604.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Get MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 2. Split Train Test Data is handled by torchvision datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPx5TcJ5IeVX",
        "outputId": "bf9d8917-6ddc-4615-9622-aa4d9ac9a763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.305242\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.234400\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.086308\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.199925\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.054832\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.075595\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.023777\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.028338\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.109093\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.031632\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.029507\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.135557\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.065370\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.020308\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.073765\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.031962\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.035444\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.051980\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.016026\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.025170\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.040233\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.011817\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.009740\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.002755\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.080399\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.007244\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.034519\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.104337\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.008148\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.066474\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.010861\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.014562\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.071484\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.019230\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.008860\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.012541\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.028535\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.023078\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.016736\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.005157\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.008193\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.019471\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.017675\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.013186\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.003534\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.082626\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.034020\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.010420\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.030578\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.003122\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001938\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.005191\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.010323\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001516\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.003974\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.010236\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.016718\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.015574\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.021734\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.011176\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.003178\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.031254\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.000522\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.002064\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.018381\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.002263\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.105069\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.003403\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.031482\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.010135\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.015253\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.048455\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.005933\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.009253\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.000944\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.064284\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.005313\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.000872\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.000664\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.000979\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001516\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001877\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001289\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.009848\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.000544\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001417\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.022076\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.000631\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.000420\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.010672\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.005927\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000881\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000593\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.008751\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001715\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000480\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000877\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000582\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002069\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.021105\n"
          ]
        }
      ],
      "source": [
        "# 3. Use torch to build a LeNet model\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = LeNet()\n",
        "\n",
        "# 4. Model Compile\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# 5. Model Fit\n",
        "def train(model, device, train_loader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "train(model, device, train_loader, criterion, optimizer, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-qbSMCcImGK",
        "outputId": "3d981383-f5b0-40dc-da49-6471dab77590"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9928/10000 (99%)\n"
          ]
        }
      ],
      "source": [
        "# 6. Model Evaluate on Test Set\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnkorGimJ9hr"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DbE7UuxIqD0",
        "outputId": "230b999f-a795-40c9-bdce-b1d94632d220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 66013578.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:06<00:00, 85.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Batch: 200, Loss: 0.7933\n",
            "Epoch: 1, Batch: 400, Loss: 0.4206\n",
            "Epoch: 1, Batch: 600, Loss: 0.3594\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Get CIFAR10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to fit VGG16 input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # CIFAR10 normalization\n",
        "])\n",
        "\n",
        "# 2. Split Train/Test Data is already handled by the CIFAR10 dataset in torchvision\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 3. Use torch to build a VGG16 model\n",
        "vgg16 = models.vgg16(pretrained=True)\n",
        "vgg16.classifier[6] = nn.Linear(4096, 10)  # Adjusting the last layer for 10 classes of CIFAR10\n",
        "\n",
        "# 4. Model Compile\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 5. Model Fit\n",
        "def train(model, device, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if batch_idx % 200 == 199:\n",
        "                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {running_loss / 200:.4f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(vgg16, device, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWUuGH66JSa3"
      },
      "outputs": [],
      "source": [
        "# 6. Model Evaluate on Test Set\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n",
        "\n",
        "evaluate(vgg16, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhgsv1whKAZa"
      },
      "source": [
        "## ResNet or Residual Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kbevlJiJv72"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntud5kLkJxQt"
      },
      "outputs": [],
      "source": [
        "# 1. Get CIFAR10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFomShWlJywv"
      },
      "outputs": [],
      "source": [
        "# 3. Use torch to build a ResNet model\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 10)  # Adjust for CIFAR10's 10 classes\n",
        "\n",
        "# 4. Model Compile\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 5. Model Fit\n",
        "def train(model, device, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(resnet, device, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MLrixLTJ0Ma"
      },
      "outputs": [],
      "source": [
        "# 6. Model Evaluate on Test Set\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
        "\n",
        "evaluate(resnet, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDCLjmGGKXNj"
      },
      "source": [
        "## DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H8JVDboKW2l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du0seO6QKY4r"
      },
      "outputs": [],
      "source": [
        "# 1. Get CIFAR10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to fit DenseNet input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize CIFAR10 images\n",
        "])\n",
        "\n",
        "# 2. Split Train/Test Data is already handled by torchvision\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BluEZDK8KcCF"
      },
      "outputs": [],
      "source": [
        "# 3. Use torch to build a DenseNet model\n",
        "densenet = models.densenet121(pretrained=True)\n",
        "densenet.classifier = nn.Linear(densenet.classifier.in_features, 10)  # Adjust for CIFAR10 classes\n",
        "\n",
        "# 4. Model Compile\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(densenet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 5. Model Fit\n",
        "def train(model, device, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(densenet, device, train_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K09XZxfZKdWG"
      },
      "outputs": [],
      "source": [
        "# 6. Model Evaluate on Test Set\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
        "\n",
        "evaluate(densenet, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Few1WXzsTDj_"
      },
      "source": [
        "## Inception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CshDU56NM1Zq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Get CIFAR10 Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(299),  # Resize to the input size expected by Inception\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 2. Split Train/Test Data is already handled by torchvision\n",
        "\n",
        "# 3. Build an Inception Model\n",
        "inception = models.inception_v3(pretrained=True)\n",
        "# Adapt the auxiliary classifier\n",
        "inception.AuxLogits.fc = nn.Linear(inception.AuxLogits.fc.in_features, 10)\n",
        "# Adapt the main classifier\n",
        "inception.fc = nn.Linear(inception.fc.in_features, 10)\n",
        "\n",
        "# 4. Model Compile\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(inception.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 5. Model Fit\n",
        "def train(model, device, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, aux_outputs = model(data)\n",
        "            loss1 = criterion(outputs, targets)\n",
        "            loss2 = criterion(aux_outputs, targets)\n",
        "            loss = loss1 + 0.4*loss2\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train(inception, device, train_loader, criterion, optimizer)\n",
        "\n",
        "# 6. Model Evaluate on Test Set\n",
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Accuracy on test set: {100 * correct / total}%')\n",
        "\n",
        "evaluate(inception, device, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}