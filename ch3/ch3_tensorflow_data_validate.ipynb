{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuSYVbwEYNHw"
      },
      "source": [
        "# TensorFlow Data Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPt5BHTwy_0F"
      },
      "source": [
        "This example colab notebook demonstrates the use of TensorFlow Data Validation (TFDV) for exploring and visualizing datasets comprehensively. This process involves examining descriptive statistics, deriving a schema, identifying and rectifying anomalies, as well as monitoring dataset drift and skew. Gaining insights into the dataset's features and its evolution over time within a production environment is crucial. Additionally, detecting data anomalies and ensuring consistency across training, evaluation, and serving datasets is vital for model reliability.\n",
        "\n",
        "The dataset utilized in this example is the [Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) provided by the City of Chicago.\n",
        "\n",
        "Note: The data for these applications has been adapted from its original form on www.cityofchicago.org, the City of Chicago's official website. The City of Chicago does not guarantee the data's accuracy, timeliness, or completeness presented here. The data is subject to change and should be used at one's own risk.\n",
        "\n",
        "To learn more about the dataset, visit [Google BigQuery](https://cloud.google.com/bigquery/public-data/chicago-taxi) and explore the dataset in detail via the [BigQuery UI](https://bigquery.cloud.google.com/dataset/bigquery-public-data:chicago_taxi_trips).\n",
        "\n",
        "Key Point: As developers and modelers, it's important to consider how this data is utilized, along with the possible positive and negative impacts of a model's predictions. A model could potentially perpetuate existing societal biases and inequalities. Evaluate whether a feature is truly pertinent to your objective or if it might introduce bias. For further insights, delve into [ML fairness](https://developers.google.com/machine-learning/fairness-overview/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnm6Mj3vTGLm"
      },
      "source": [
        "The columns in the dataset are:\n",
        "<table>\n",
        "<tr><td>pickup_community_area</td><td>fare</td><td>trip_start_month</td></tr>\n",
        "\n",
        "<tr><td>trip_start_hour</td><td>trip_start_day</td><td>trip_start_timestamp</td></tr>\n",
        "<tr><td>pickup_latitude</td><td>pickup_longitude</td><td>dropoff_latitude</td></tr>\n",
        "<tr><td>dropoff_longitude</td><td>trip_miles</td><td>pickup_census_tract</td></tr>\n",
        "<tr><td>dropoff_census_tract</td><td>payment_type</td><td>company</td></tr>\n",
        "<tr><td>trip_seconds</td><td>dropoff_community_area</td><td>tips</td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsYC3O-DnYro"
      },
      "source": [
        "## Install and import packages\n",
        "\n",
        "Install the packages for TensorFlow Data Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATF_g5c2g2Ha"
      },
      "source": [
        "### Upgrade Pip\n",
        "\n",
        "To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0ISmRq3nY3-"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qBFH1ARcSNk"
      },
      "source": [
        "### Install Data Validation packages\n",
        "\n",
        "Install the TensorFlow Data Validation packages and dependencies, which takes a few minutes. You may see warnings and errors regarding incompatible dependency versions, which you will resolve in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPJsE5Gkdp8m"
      },
      "outputs": [],
      "source": [
        "print('Installing TensorFlow Data Validation')\n",
        "!pip install --upgrade 'tensorflow_data_validation[visualization]<2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_NXX5GaSiZx"
      },
      "source": [
        "### Import TensorFlow and reload updated packages\n",
        "\n",
        "The previous action upgrades the default packages within the Google Colab environment, necessitating a refresh of the package resources to accommodate the updated dependencies.\n",
        "\n",
        "Note: Undertaking this step addresses the dependency issues stemming from the installation process. Should you continue to face issues with code execution post this update, consider restarting the runtime (via Runtime > Restart runtime ...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2j9VD9HbGWw"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import importlib\n",
        "importlib.reload(pkg_resources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFXK2AdpSpv0"
      },
      "source": [
        "Check the versions of TensorFlow and the Data Validation before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5rPatTDSCHB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "print('TF version:', tf.__version__)\n",
        "print('TFDV version:', tfdv.version.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MizoHg1DRlK"
      },
      "source": [
        "## Load the dataset\n",
        "We will download our dataset from Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5gfFiTeDa6Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile, urllib, zipfile\n",
        "\n",
        "# Set up some globals for our file paths\n",
        "BASE_DIR = tempfile.mkdtemp()\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output')\n",
        "TRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\n",
        "EVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\n",
        "SERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n",
        "\n",
        "# Download the zip file from GCP and unzip it\n",
        "zip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\n",
        "zipfile.ZipFile(zip).extractall(BASE_DIR)\n",
        "zipfile.ZipFile(zip).close()\n",
        "\n",
        "print(\"Here's what we downloaded:\")\n",
        "!ls -R {os.path.join(BASE_DIR, 'data')}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0sFmiTbT8-x"
      },
      "source": [
        "## Compute and visualize statistics\n",
        "\n",
        "Initially, we'll employ [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to calculate statistics for our training dataset. (Please disregard any snappy warnings).\n",
        "\n",
        "TFDV is capable of generating [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that offer a snapshot overview of the dataset, highlighting available features and the distribution of their values.\n",
        "\n",
        "For the computation of statistics across potentially vast datasets, TFDV leverages the [Apache Beam](https://beam.apache.org/) data-parallel processing framework, ensuring scalability. Additionally, for those looking to deeply integrate TFDV into their workflows (for instance, generating statistics as part of a data-production pipeline), TFDV provides access to a Beam PTransform specifically for the creation of statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE481oMbT-H0"
      },
      "outputs": [],
      "source": [
        "train_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhXQSxJ2dB_6"
      },
      "source": [
        "Let's proceed to utilize [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which employs [Facets](https://pair-code.github.io/facets/) for generating a clear visual representation of our training dataset:\n",
        "\n",
        "* Observe that numeric and categorical features are depicted separately, with individual charts illustrating the distribution of each feature.\n",
        "* Pay attention to features exhibiting missing or null values, which are highlighted in red percentages to signal potential issues with data integrity in those features. The indicated percentage reflects the proportion of data points lacking or having null values for a given feature.\n",
        "* Note the absence of data for `pickup_census_tract`, suggesting a chance to reduce the dataset's dimensionality.\n",
        "* Experiment by clicking \"expand\" above the charts to alter the visualization.\n",
        "* Explore the data further by hovering over the bars within the charts to reveal the ranges and counts of each bucket.\n",
        "* Toggle between log and linear scales to observe how the log scale can unveil more intricate details about the `payment_type` categorical feature.\n",
        "* Consider selecting \"quantiles\" from the \"Chart to show\" dropdown and hover over the markers to view the corresponding quantile percentages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3tUKgh7Up3x"
      },
      "outputs": [],
      "source": [
        "# docs-infra: no-execute\n",
        "tfdv.visualize_statistics(train_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoc0ijE5LYeQ"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/statistics.png\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVR02-y4V0uM"
      },
      "source": [
        "## Infer a schema\n",
        "\n",
        "Next, we'll employ [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to generate a schema for our dataset. A schema outlines the necessary constraints for machine learning data, such as the data type for each feature (numerical or categorical), presence regularity, and, for categorical features, the domain - a set of permissible values. Given the complexity of manually crafting a schema for data-rich datasets, TFDV automates this process by producing an initial schema draft based on the descriptive statistics previously computed.\n",
        "\n",
        "Ensuring the accuracy of the schema is critical, as it underpins the integrity of the entire production pipeline. Beyond serving as a cornerstone for data validation, the schema acts as a form of documentation, facilitating collaboration among developers working with the dataset. To review and validate the inferred schema, let's visualize it using [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LLkRJThVr9m"
      },
      "outputs": [],
      "source": [
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "tfdv.display_schema(schema=schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVa3EXE8WEDE"
      },
      "source": [
        "## Check evaluation data for errors\n",
        "\n",
        "Until now, our focus has been exclusively on the training data. However, it's crucial that our evaluation data aligns with our training data, including adherence to the same schema. Equally important is ensuring that the evaluation data reflects a similar range of values for our numerical features as found in the training data, to guarantee that the evaluation encompasses a comparable span of the loss surface as training does. This principle applies to categorical features as well. A lack of consistency can lead to unidentified training issues during evaluation if parts of the loss surface are not examined.\n",
        "\n",
        "* Observe that statistics now cover both training and evaluation data for each feature.\n",
        "* Charts are updated to overlay data from both training and evaluation sets, simplifying comparison.\n",
        "* A new percentages view has been added to the charts, which works with either log or the standard linear scales.\n",
        "* Differences in the mean and median values for `trip_miles` between training and evaluation data raise questions about potential issues.\n",
        "* A significant discrepancy in the maximum `tips` between training and evaluation datasets prompts concerns over possible problems.\n",
        "* Expanding the Numeric Features chart and switching to the log scale reveals a notable difference in the maximum for `trip_seconds`. Does this mean the evaluation might overlook certain areas of the loss surface?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_P0RLYlV6XG"
      },
      "outputs": [],
      "source": [
        "# Compute stats for evaluation data\n",
        "eval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn-3fQWJLimn"
      },
      "outputs": [],
      "source": [
        "# docs-infra: no-execute\n",
        "# Compare evaluation data with training data\n",
        "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
        "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS4u82lzLeRh"
      },
      "source": [
        "<!-- <img class=\"tfo-display-only-on-site\" src=\"images/statistics_eval.png\"/> -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycRRa4leHp84"
      },
      "source": [
        "## Check for evaluation anomalies\n",
        "\n",
        "Is the evaluation dataset consistent with the schema of our training dataset? This consideration is particularly critical for categorical features, as it's essential to ascertain the spectrum of permissible values.\n",
        "\n",
        "Crucial Insight: Consider the implications of evaluating with data that includes categorical feature values absent from our training dataset. Similarly, ponder the effects of numeric features possessing values beyond the scopes observed in our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7uGVeL2WOam"
      },
      "outputs": [],
      "source": [
        "# Check eval data for errors by validating the eval data stats using the previously inferred schema.\n",
        "anomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
        "tfdv.display_anomalies(anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzxx1gBpJIBa"
      },
      "source": [
        "## Fix evaluation anomalies in the schema\n",
        "\n",
        "It appears we've encountered new values for `company` and `payment_type` in our evaluation data that weren't present in our training dataset. These should be flagged as anomalies, but the approach to addressing them hinges on our understanding of the data. If these anomalies signify data inaccuracies, corrections should be made to the data itself. Alternatively, if they are valid within the context of the data, updating the schema to encompass these new evaluation dataset values is a viable solution.\n",
        "\n",
        "Critical Consideration: How might our evaluation outcomes be impacted if we overlook these discrepancies?\n",
        "\n",
        "While not all issues can be rectified by modifying the evaluation dataset, adjustments within the schema are possible for anomalies we deem acceptable. This may involve redefining our criteria for anomalies for specific features and amending the schema to account for previously unlisted values for categorical features. TFDV's insights have illuminated the necessary corrections.\n",
        "\n",
        "Now, let's proceed with those corrections and conduct a final review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "legN2nXLWZAc"
      },
      "outputs": [],
      "source": [
        "# Relax the minimum fraction of values that must come from the domain for feature company.\n",
        "company = tfdv.get_feature(schema, 'company')\n",
        "company.distribution_constraints.min_domain_mass = 0.9\n",
        "\n",
        "# Add new value to the domain of feature payment_type.\n",
        "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
        "payment_type_domain.value.append('Prcard')\n",
        "\n",
        "# Validate eval stats after updating the schema\n",
        "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
        "tfdv.display_anomalies(updated_anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNo72YP9LN98"
      },
      "source": [
        "Hey, look at that!  We verified that the training and evaluation data are now consistent!  Thanks TFDV ;)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ1P4ucHJj5o"
      },
      "source": [
        "## Schema Environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb179jczJppA"
      },
      "source": [
        "In this scenario, we've also allocated a 'serving' dataset for demonstration purposes, which warrants its own verification. Typically, all datasets within a pipeline are expected to adhere to the same schema, though exceptions are common. For instance, supervised learning models include labels in their datasets for training, but these labels are omitted during model inference in serving. Sometimes, slight modifications to the schema are necessary to accommodate these differences.\n",
        "\n",
        "**Environments** offer a way to manage such specific needs. They allow for the association of features within the schema to different sets of conditions using `default_environment`, `in_environment`, and `not_in_environment`.\n",
        "\n",
        "Taking this dataset as an example, the `tips` feature serves as the label during training but is absent in the serving data. Without defining an appropriate environment, this discrepancy would be flagged as an anomaly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSZfbnifJuTA"
      },
      "outputs": [],
      "source": [
        "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDYHvZ09LfkT"
      },
      "source": [
        "We'll address the `tips` feature shortly. Additionally, we've encountered an issue where the `trip seconds` feature, which our schema anticipated to be a FLOAT, is actually an INT. TFDV plays a crucial role here by highlighting this discrepancy, pointing out potential inconsistencies in how data is prepared for training versus serving. Such discrepancies can go unnoticed until they impact model performance, potentially in severe ways. Whether this issue is critical or not warrants further examination.\n",
        "\n",
        "In this scenario, converting INT values to FLOATs is a straightforward solution, and we aim to instruct TFDV to align with our schema for type inference. Let's proceed with this adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhtYF8aAczpd"
      },
      "outputs": [],
      "source": [
        "options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n",
        "serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)\n",
        "serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJjh5rigc5xy"
      },
      "source": [
        "Now we just have the `tips` feature (which is our label) showing up as an anomaly ('Column dropped').  Of course we don't expect to have labels in our serving data, so let's tell TFDV to ignore that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnbnw8H6Lp2M"
      },
      "outputs": [],
      "source": [
        "# All features are by default in both TRAINING and SERVING environments.\n",
        "schema.default_environment.append('TRAINING')\n",
        "schema.default_environment.append('SERVING')\n",
        "\n",
        "# Specify that 'tips' feature is not in SERVING environment.\n",
        "tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n",
        "\n",
        "serving_anomalies_with_env = tfdv.validate_statistics(\n",
        "    serving_stats, schema, environment='SERVING')\n",
        "\n",
        "tfdv.display_anomalies(serving_anomalies_with_env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yteMr3AGMYEp"
      },
      "source": [
        "## Check for drift and skew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ftd5k6AMkPV"
      },
      "source": [
        "TFDV goes beyond merely verifying dataset compliance with the predefined schema; it is also equipped to identify drift and skew across datasets. This is accomplished by contrasting the statistical properties of various datasets, guided by drift/skew comparators integrated into the schema.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drift\n",
        "\n",
        "For categorical features, TFDV facilitates drift detection across sequential data spans (e.g., from span N to span N+1), which might represent successive days of training data. Drift is quantified using the [L-infinity distance](https://en.wikipedia.org/wiki/Chebyshev_distance), allowing users to establish a threshold beyond which an alert for excessive drift would be triggered. Determining an appropriate threshold typically demands a blend of domain expertise and iterative experimentation."
      ],
      "metadata": {
        "id": "QUutXfeQhS3c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBFuLpXb6qSp"
      },
      "source": [
        "### Skew\n",
        "\n",
        "TFDV is adept at identifying three distinct types of skew in data: schema skew, feature skew, and distribution skew.\n",
        "\n",
        "#### Schema Skew\n",
        "\n",
        "Schema skew arises when there's a discrepancy in schema adherence between the training and serving datasets. Both sets are expected to follow the same schema. Any deviations, such as the presence of a label feature in training data but its absence in serving data, should be clearly defined using the environments field within the schema.\n",
        "\n",
        "#### Feature Skew\n",
        "\n",
        "Feature skew is noted when there's a difference between the features on which a model was trained and the features it encounters during serving. This could occur under circumstances like:\n",
        "\n",
        "* Alterations in a data source that supplies certain feature values from the training phase to the serving phase.\n",
        "* Discrepancies in feature generation logic across training and serving, such as applying a transformation exclusively in one environment.\n",
        "\n",
        "#### Distribution Skew\n",
        "\n",
        "Distribution skew is observed when the training dataset's distribution diverges significantly from that of the serving dataset. This skew can result from different methodologies or data sources used in creating the training set or from a flawed sampling method that selects a non-representative subset of the serving data for training purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEUsZm_rOd1Q"
      },
      "outputs": [],
      "source": [
        "# Add skew comparator for 'payment_type' feature.\n",
        "payment_type = tfdv.get_feature(schema, 'payment_type')\n",
        "payment_type.skew_comparator.infinity_norm.threshold = 0.01\n",
        "\n",
        "# Add drift comparator for 'company' feature.\n",
        "company=tfdv.get_feature(schema, 'company')\n",
        "company.drift_comparator.infinity_norm.threshold = 0.001\n",
        "\n",
        "skew_anomalies = tfdv.validate_statistics(train_stats, schema,\n",
        "                                          previous_statistics=eval_stats,\n",
        "                                          serving_statistics=serving_stats)\n",
        "\n",
        "tfdv.display_anomalies(skew_anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GzbbsPgf0Bg"
      },
      "source": [
        "In this example we do see some drift, but it is well below the threshold that we've set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ5saC9eWvHx"
      },
      "source": [
        "## Freeze the schema\n",
        "\n",
        "Now that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydkL4DkIWn18"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.lib.io import file_io\n",
        "from google.protobuf import text_format\n",
        "\n",
        "file_io.recursive_create_dir(OUTPUT_DIR)\n",
        "schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\n",
        "tfdv.write_schema_text(schema, schema_file)\n",
        "\n",
        "!cat {schema_file}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8eC59yISdGB"
      },
      "source": [
        "## When to use TFDV\n",
        "\n",
        "While it might seem like the role of TFDV is confined to the initial stages of your training pipeline, as illustrated in this guide, its utility extends far beyond that. Here are a few additional applications:\n",
        "\n",
        "* Ensuring the quality of new data entering the inference pipeline by verifying that no incorrect features are introduced.\n",
        "* Confirming that new data for inference covers aspects of the decision surface that the model has been trained on.\n",
        "* Checking the integrity of data post-transformation and feature engineering (often conducted using [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)) to prevent and identify any errors in the process."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QFA1OEDihmxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}